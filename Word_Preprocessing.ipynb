{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovHcJihbYHJu",
        "outputId": "2359710a-e3a8-4ba6-fd1f-82b7ae3d23cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique English words: ['ie', 'ai', 'j', 'le', 'ha', 'wa', 'eg', '14']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words as nltk_words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "\n",
        "# URL of the webpage to extract text from\n",
        "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
        "\n",
        "# Retrieve HTML content from the URL\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "\n",
        "# parse HTML using BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Extract text from paragraphs and headings tags\n",
        "text = ' '.join([tag.get_text() for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])])\n",
        "\n",
        "# Clean data by removing symbols and special characters\n",
        "cleaned_text = re.sub(r'[^a-zA-Z0-9_\\s]', '', text)\n",
        "\n",
        "# Normalize text to lowercase\n",
        "cleaned_text = cleaned_text.lower()\n",
        "\n",
        "# Tokenize the text into words\n",
        "words_list = word_tokenize(cleaned_text)\n",
        "\n",
        "# Lemmatize each word to its base form\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words_list]\n",
        "\n",
        "# Get English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the text\n",
        "filtered_words = [word for word in lemmatized_words if word not in stop_words]\n",
        "\n",
        "# Get unique words from the list\n",
        "unique_words = set(filtered_words)\n",
        "\n",
        "english_words = set(nltk_words.words())\n",
        "\n",
        "unique_words = [word for word in unique_words if len(word) < 3]\n",
        "\n",
        "print(\"Unique English words:\", unique_words)"
      ]
    }
  ]
}